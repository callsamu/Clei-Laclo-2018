{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "New-Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4aj22fZhLKPlx9GaSF1J6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sousam/Clei-Laclo-2018/blob/master/New_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTwvDzEtTed_",
        "outputId": "1f667023-d499-4489-dcfa-4dec5bc40ce9"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwYbPBooNvlo"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import datasets\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras import Sequential\r\n",
        "from tensorflow.keras.layers import Conv1D,AveragePooling1D,Flatten,Dense\r\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import ReLU\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.utils import plot_model\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import sklearn\r\n",
        "from sklearn import preprocessing, decomposition, model_selection\r\n",
        "#tf.compat.v1.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztROL98rTlQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d743b4c-47e8-4ae9-ef0f-df3a8e6db854"
      },
      "source": [
        "#loading the data\r\n",
        "data = pd.read_table('australian.dat', sep=\"\\s+\", header=None).dropna()\r\n",
        "#data = pd.read_csv('australian-encoded.csv',header=0)\r\n",
        "print(data)\r\n",
        "x = data[data.columns[:-1]].to_numpy()\r\n",
        "y = data[data.columns[-1]].to_numpy()\r\n",
        "\r\n",
        "x.reshape(x.shape[0],x.shape[1],1)\r\n",
        "  \r\n",
        "#Scaling\r\n",
        "scaler = preprocessing.MinMaxScaler()\r\n",
        "    \r\n",
        "scaled_data = scaler.fit_transform(x)\r\n",
        "  \r\n",
        "#75% percent of dataset is designated to training\r\n",
        "X_train, X_test = model_selection.train_test_split(scaled_data, test_size=0.2)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     0      1       2   3   4   5      6   7   8   9   10  11   12    13  14\n",
            "0     1  22.08  11.460   2   4   4  1.585   0   0   0   1   2  100  1213   0\n",
            "1     0  22.67   7.000   2   8   4  0.165   0   0   0   0   2  160     1   0\n",
            "2     0  29.58   1.750   1   4   4  1.250   0   0   0   1   2  280     1   0\n",
            "3     0  21.67  11.500   1   5   3  0.000   1   1  11   1   2    0     1   1\n",
            "4     1  20.17   8.170   2   6   4  1.960   1   1  14   0   2   60   159   1\n",
            "..   ..    ...     ...  ..  ..  ..    ...  ..  ..  ..  ..  ..  ...   ...  ..\n",
            "685   1  31.57  10.500   2  14   4  6.500   1   0   0   0   2    0     1   1\n",
            "686   1  20.67   0.415   2   8   4  0.125   0   0   0   0   2    0    45   0\n",
            "687   0  18.83   9.540   2   6   4  0.085   1   0   0   0   2  100     1   1\n",
            "688   0  27.42  14.500   2  14   8  3.085   1   1   1   0   2  120    12   1\n",
            "689   1  41.00   0.040   2  10   4  0.040   0   1   1   0   1  560     1   1\n",
            "\n",
            "[690 rows x 15 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2triuKUOTqvk"
      },
      "source": [
        "epochs = 100\r\n",
        "batch_size = len(X_test)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9sfKuxacRU-"
      },
      "source": [
        "n_inputs = len(X_train[0])\r\n",
        "# define encoder\r\n",
        "visible = Input(shape=(n_inputs,))\r\n",
        "# encoder level 1\r\n",
        "e = Dense(n_inputs*2)(visible)\r\n",
        "#e = BatchNormalization()(e)\r\n",
        "e = ReLU()(e)\r\n",
        "# encoder level 2\r\n",
        "e = Dense(n_inputs)(e)\r\n",
        "#e = BatchNormalization()(e)\r\n",
        "e = ReLU()(e)\r\n",
        "# bottleneck\r\n",
        "n_bottleneck = 10\r\n",
        "bottleneck = Dense(n_bottleneck, activation='sigmoid')(e)\r\n",
        "# define decoder, level 1\r\n",
        "d = Dense(n_inputs)(bottleneck)\r\n",
        "#d = BatchNormalization()(d)\r\n",
        "d = ReLU()(d)\r\n",
        "# decoder level 2\r\n",
        "d = Dense(n_inputs*2)(d)\r\n",
        "#d = BatchNormalization()(d)\r\n",
        "d = ReLU()(d)\r\n",
        "# output layer\r\n",
        "output = Dense(n_inputs, activation='linear')(d)\r\n",
        "# define autoencoder model\r\n",
        "model = Model(inputs=visible, outputs=output)\r\n",
        "\r\n",
        "#vector_loss = tf.losses.mean_squared_error(visible, output, reduction=tf.losses.Reduction.NONE)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpGSb4Cmd4fn"
      },
      "source": [
        "opt = tf.keras.optimizers.SGD()\r\n",
        "\r\n",
        "vector_loss = tf.keras.losses.MeanSquaredError(reduction=tf.losses.Reduction.NONE)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uES049Dkd-kp"
      },
      "source": [
        "model.compile(optimizer=opt, loss=vector_loss)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93v_bU8geHQn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f832f4-fa9c-4472-ff19-48da8bb602db"
      },
      "source": [
        "history = model.fit(X_train, X_train, batch_size=batch_size, epochs=epochs, verbose=2, validation_data=(X_test,X_test))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 552 samples, validate on 138 samples\n",
            "Epoch 1/100\n",
            "552/552 - 1s - loss: 0.1511 - val_loss: 0.0940\n",
            "Epoch 2/100\n",
            "552/552 - 0s - loss: 0.0920 - val_loss: 0.0904\n",
            "Epoch 3/100\n",
            "552/552 - 0s - loss: 0.0911 - val_loss: 0.0902\n",
            "Epoch 4/100\n",
            "552/552 - 0s - loss: 0.0908 - val_loss: 0.0893\n",
            "Epoch 5/100\n",
            "552/552 - 0s - loss: 0.0909 - val_loss: 0.0890\n",
            "Epoch 6/100\n",
            "552/552 - 0s - loss: 0.0909 - val_loss: 0.0895\n",
            "Epoch 7/100\n",
            "552/552 - 0s - loss: 0.0907 - val_loss: 0.0898\n",
            "Epoch 8/100\n",
            "552/552 - 0s - loss: 0.0906 - val_loss: 0.0895\n",
            "Epoch 9/100\n",
            "552/552 - 0s - loss: 0.0907 - val_loss: 0.0900\n",
            "Epoch 10/100\n",
            "552/552 - 0s - loss: 0.0906 - val_loss: 0.0899\n",
            "Epoch 11/100\n",
            "552/552 - 0s - loss: 0.0909 - val_loss: 0.0894\n",
            "Epoch 12/100\n",
            "552/552 - 0s - loss: 0.0905 - val_loss: 0.0900\n",
            "Epoch 13/100\n",
            "552/552 - 0s - loss: 0.0907 - val_loss: 0.0904\n",
            "Epoch 14/100\n",
            "552/552 - 0s - loss: 0.0905 - val_loss: 0.0899\n",
            "Epoch 15/100\n",
            "552/552 - 0s - loss: 0.0904 - val_loss: 0.0886\n",
            "Epoch 16/100\n",
            "552/552 - 0s - loss: 0.0904 - val_loss: 0.0900\n",
            "Epoch 17/100\n",
            "552/552 - 0s - loss: 0.0903 - val_loss: 0.0891\n",
            "Epoch 18/100\n",
            "552/552 - 0s - loss: 0.0905 - val_loss: 0.0891\n",
            "Epoch 19/100\n",
            "552/552 - 0s - loss: 0.0904 - val_loss: 0.0886\n",
            "Epoch 20/100\n",
            "552/552 - 0s - loss: 0.0902 - val_loss: 0.0898\n",
            "Epoch 21/100\n",
            "552/552 - 0s - loss: 0.0900 - val_loss: 0.0885\n",
            "Epoch 22/100\n",
            "552/552 - 0s - loss: 0.0898 - val_loss: 0.0878\n",
            "Epoch 23/100\n",
            "552/552 - 0s - loss: 0.0891 - val_loss: 0.0866\n",
            "Epoch 24/100\n",
            "552/552 - 0s - loss: 0.0888 - val_loss: 0.0868\n",
            "Epoch 25/100\n",
            "552/552 - 0s - loss: 0.0877 - val_loss: 0.0861\n",
            "Epoch 26/100\n",
            "552/552 - 0s - loss: 0.0869 - val_loss: 0.0848\n",
            "Epoch 27/100\n",
            "552/552 - 0s - loss: 0.0860 - val_loss: 0.0827\n",
            "Epoch 28/100\n",
            "552/552 - 0s - loss: 0.0839 - val_loss: 0.0806\n",
            "Epoch 29/100\n",
            "552/552 - 0s - loss: 0.0825 - val_loss: 0.0792\n",
            "Epoch 30/100\n",
            "552/552 - 0s - loss: 0.0801 - val_loss: 0.0789\n",
            "Epoch 31/100\n",
            "552/552 - 0s - loss: 0.0836 - val_loss: 0.0968\n",
            "Epoch 32/100\n",
            "552/552 - 0s - loss: 0.0819 - val_loss: 0.0803\n",
            "Epoch 33/100\n",
            "552/552 - 0s - loss: 0.0807 - val_loss: 0.0918\n",
            "Epoch 34/100\n",
            "552/552 - 0s - loss: 0.0783 - val_loss: 0.0743\n",
            "Epoch 35/100\n",
            "552/552 - 0s - loss: 0.0777 - val_loss: 0.0779\n",
            "Epoch 36/100\n",
            "552/552 - 0s - loss: 0.0751 - val_loss: 0.0811\n",
            "Epoch 37/100\n",
            "552/552 - 0s - loss: 0.0783 - val_loss: 0.0678\n",
            "Epoch 38/100\n",
            "552/552 - 0s - loss: 0.0705 - val_loss: 0.0805\n",
            "Epoch 39/100\n",
            "552/552 - 0s - loss: 0.0734 - val_loss: 0.0823\n",
            "Epoch 40/100\n",
            "552/552 - 0s - loss: 0.0770 - val_loss: 0.0713\n",
            "Epoch 41/100\n",
            "552/552 - 0s - loss: 0.0701 - val_loss: 0.0715\n",
            "Epoch 42/100\n",
            "552/552 - 0s - loss: 0.0691 - val_loss: 0.0711\n",
            "Epoch 43/100\n",
            "552/552 - 0s - loss: 0.0694 - val_loss: 0.0689\n",
            "Epoch 44/100\n",
            "552/552 - 0s - loss: 0.0681 - val_loss: 0.0668\n",
            "Epoch 45/100\n",
            "552/552 - 0s - loss: 0.0658 - val_loss: 0.0650\n",
            "Epoch 46/100\n",
            "552/552 - 0s - loss: 0.0669 - val_loss: 0.0691\n",
            "Epoch 47/100\n",
            "552/552 - 0s - loss: 0.0689 - val_loss: 0.0693\n",
            "Epoch 48/100\n",
            "552/552 - 0s - loss: 0.0668 - val_loss: 0.0643\n",
            "Epoch 49/100\n",
            "552/552 - 0s - loss: 0.0654 - val_loss: 0.0647\n",
            "Epoch 50/100\n",
            "552/552 - 0s - loss: 0.0649 - val_loss: 0.0647\n",
            "Epoch 51/100\n",
            "552/552 - 0s - loss: 0.0649 - val_loss: 0.0644\n",
            "Epoch 52/100\n",
            "552/552 - 0s - loss: 0.0642 - val_loss: 0.0634\n",
            "Epoch 53/100\n",
            "552/552 - 0s - loss: 0.0639 - val_loss: 0.0630\n",
            "Epoch 54/100\n",
            "552/552 - 0s - loss: 0.0639 - val_loss: 0.0633\n",
            "Epoch 55/100\n",
            "552/552 - 0s - loss: 0.0643 - val_loss: 0.0633\n",
            "Epoch 56/100\n",
            "552/552 - 0s - loss: 0.0643 - val_loss: 0.0639\n",
            "Epoch 57/100\n",
            "552/552 - 0s - loss: 0.0645 - val_loss: 0.0638\n",
            "Epoch 58/100\n",
            "552/552 - 0s - loss: 0.0648 - val_loss: 0.0639\n",
            "Epoch 59/100\n",
            "552/552 - 0s - loss: 0.0641 - val_loss: 0.0636\n",
            "Epoch 60/100\n",
            "552/552 - 0s - loss: 0.0641 - val_loss: 0.0638\n",
            "Epoch 61/100\n",
            "552/552 - 0s - loss: 0.0646 - val_loss: 0.0648\n",
            "Epoch 62/100\n",
            "552/552 - 0s - loss: 0.0659 - val_loss: 0.0643\n",
            "Epoch 63/100\n",
            "552/552 - 0s - loss: 0.0645 - val_loss: 0.0632\n",
            "Epoch 64/100\n",
            "552/552 - 0s - loss: 0.0643 - val_loss: 0.0626\n",
            "Epoch 65/100\n",
            "552/552 - 0s - loss: 0.0634 - val_loss: 0.0619\n",
            "Epoch 66/100\n",
            "552/552 - 0s - loss: 0.0634 - val_loss: 0.0616\n",
            "Epoch 67/100\n",
            "552/552 - 0s - loss: 0.0628 - val_loss: 0.0614\n",
            "Epoch 68/100\n",
            "552/552 - 0s - loss: 0.0626 - val_loss: 0.0614\n",
            "Epoch 69/100\n",
            "552/552 - 0s - loss: 0.0627 - val_loss: 0.0610\n",
            "Epoch 70/100\n",
            "552/552 - 0s - loss: 0.0623 - val_loss: 0.0616\n",
            "Epoch 71/100\n",
            "552/552 - 0s - loss: 0.0625 - val_loss: 0.0624\n",
            "Epoch 72/100\n",
            "552/552 - 0s - loss: 0.0626 - val_loss: 0.0606\n",
            "Epoch 73/100\n",
            "552/552 - 0s - loss: 0.0631 - val_loss: 0.0604\n",
            "Epoch 74/100\n",
            "552/552 - 0s - loss: 0.0623 - val_loss: 0.0623\n",
            "Epoch 75/100\n",
            "552/552 - 0s - loss: 0.0630 - val_loss: 0.0612\n",
            "Epoch 76/100\n",
            "552/552 - 0s - loss: 0.0631 - val_loss: 0.0616\n",
            "Epoch 77/100\n",
            "552/552 - 0s - loss: 0.0640 - val_loss: 0.0620\n",
            "Epoch 78/100\n",
            "552/552 - 0s - loss: 0.0652 - val_loss: 0.0620\n",
            "Epoch 79/100\n",
            "552/552 - 0s - loss: 0.0643 - val_loss: 0.0621\n",
            "Epoch 80/100\n",
            "552/552 - 0s - loss: 0.0632 - val_loss: 0.0619\n",
            "Epoch 81/100\n",
            "552/552 - 0s - loss: 0.0627 - val_loss: 0.0616\n",
            "Epoch 82/100\n",
            "552/552 - 0s - loss: 0.0639 - val_loss: 0.0615\n",
            "Epoch 83/100\n",
            "552/552 - 0s - loss: 0.0614 - val_loss: 0.0593\n",
            "Epoch 84/100\n",
            "552/552 - 0s - loss: 0.0610 - val_loss: 0.0596\n",
            "Epoch 85/100\n",
            "552/552 - 0s - loss: 0.0615 - val_loss: 0.0660\n",
            "Epoch 86/100\n",
            "552/552 - 0s - loss: 0.0611 - val_loss: 0.0582\n",
            "Epoch 87/100\n",
            "552/552 - 0s - loss: 0.0588 - val_loss: 0.0569\n",
            "Epoch 88/100\n",
            "552/552 - 0s - loss: 0.0582 - val_loss: 0.0567\n",
            "Epoch 89/100\n",
            "552/552 - 0s - loss: 0.0576 - val_loss: 0.0562\n",
            "Epoch 90/100\n",
            "552/552 - 0s - loss: 0.0585 - val_loss: 0.0623\n",
            "Epoch 91/100\n",
            "552/552 - 0s - loss: 0.0707 - val_loss: 0.0594\n",
            "Epoch 92/100\n",
            "552/552 - 0s - loss: 0.0604 - val_loss: 0.0590\n",
            "Epoch 93/100\n",
            "552/552 - 0s - loss: 0.0610 - val_loss: 0.0592\n",
            "Epoch 94/100\n",
            "552/552 - 0s - loss: 0.0597 - val_loss: 0.0595\n",
            "Epoch 95/100\n",
            "552/552 - 0s - loss: 0.0603 - val_loss: 0.0628\n",
            "Epoch 96/100\n",
            "552/552 - 0s - loss: 0.0611 - val_loss: 0.0665\n",
            "Epoch 97/100\n",
            "552/552 - 0s - loss: 0.0634 - val_loss: 0.0655\n",
            "Epoch 98/100\n",
            "552/552 - 0s - loss: 0.0601 - val_loss: 0.0634\n",
            "Epoch 99/100\n",
            "552/552 - 0s - loss: 0.0601 - val_loss: 0.0636\n",
            "Epoch 100/100\n",
            "552/552 - 0s - loss: 0.0583 - val_loss: 0.0571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHLNNrUSADw9"
      },
      "source": [
        "encoder = Model(inputs=visible, outputs=bottleneck)\r\n",
        "\r\n",
        "X_encode = encoder.predict(x)\r\n",
        "\r\n",
        "encodedData = pd.DataFrame(data=X_encode)\r\n",
        "\r\n",
        "#Adding a label to the encoded dataframe\r\n",
        "encodedData['label'] = y"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpNGxkR8EO9H",
        "outputId": "781ae864-e382-4ffe-b101-5d4827a8b195"
      },
      "source": [
        "print(encodedData)\r\n",
        "\r\n",
        "encodedData.to_csv('new-australian-encoded.csv',index=False)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            0         1         2  ...         8         9  label\n",
            "0    1.000000  1.000000  0.000000  ...  0.000000  0.000000      0\n",
            "1    0.999987  0.203034  0.999631  ...  0.999359  0.006312      0\n",
            "2    1.000000  0.010035  0.999998  ...  0.999997  0.000038      0\n",
            "3    1.000000  0.998759  0.031508  ...  0.999717  0.000004      1\n",
            "4    1.000000  1.000000  0.000000  ...  0.000315  0.000000      1\n",
            "..        ...       ...       ...  ...       ...       ...    ...\n",
            "685  1.000000  0.999545  0.179186  ...  0.998400  0.000064      1\n",
            "686  1.000000  0.999866  0.000116  ...  0.000007  0.001357      0\n",
            "687  0.998341  0.341659  0.996334  ...  0.995204  0.065052      1\n",
            "688  0.999976  0.965075  0.995087  ...  0.995371  0.003991      1\n",
            "689  1.000000  0.000018  1.000000  ...  1.000000  0.000000      1\n",
            "\n",
            "[690 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u28DInWREQzP",
        "outputId": "69f872a1-bf6b-412a-a39d-2c2398f47293"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "scaler2 = preprocessing.MinMaxScaler()\r\n",
        "    \r\n",
        "encodedData.iloc[:,:-1] = scaler2.fit_transform(encodedData.iloc[:,:-1])\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(encodedData.iloc[:,:-1], encodedData.iloc[:,-1], test_size=0.33, random_state=1)\r\n",
        "model = MLPClassifier()\r\n",
        "# fit the model on the training set\r\n",
        "model.fit(X_train, y_train)\r\n",
        "# make predictions on the test set\r\n",
        "yhat = model.predict(X_test)\r\n",
        "# calculate classification accuracy\r\n",
        "acc = accuracy_score(y_test, yhat)\r\n",
        "print(acc)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7105263157894737\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmyhM6o4GAuU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}